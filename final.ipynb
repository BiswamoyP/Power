{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import numpy as np \n",
    "from scipy.stats import randint\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "import itertools\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras import losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('household_power_consumption.txt', sep=';', \n",
    "                 parse_dates={'dt' : ['Date', 'Time']}, infer_datetime_format=True, \n",
    "                 low_memory=False, na_values=['nan','?'], index_col='dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "droping_list_all=[]\n",
    "for j in range(0,7):\n",
    "    if not df.iloc[:, j].notnull().all():\n",
    "        droping_list_all.append(j)        \n",
    "        \n",
    "droping_list_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,7):        \n",
    "        df.iloc[:,j]=df.iloc[:,j].fillna(df.iloc[:,j].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Global_active_power      0\n",
       "Global_reactive_power    0\n",
       "Voltage                  0\n",
       "Global_intensity         0\n",
       "Sub_metering_1           0\n",
       "Sub_metering_2           0\n",
       "Sub_metering_3           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdff = pd.DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(dff.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(dff.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34589, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resample = df.resample('h').mean() \n",
    "df_resample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)  var6(t-1)  \\\n",
      "1   0.636816   0.295738   0.337945   0.631157        0.0   0.011366   \n",
      "2   0.545045   0.103358   0.335501   0.541487        0.0   0.144652   \n",
      "3   0.509006   0.110073   0.283802   0.502152        0.0   0.030869   \n",
      "4   0.488550   0.096987   0.315987   0.481110        0.0   0.000000   \n",
      "5   0.455597   0.099010   0.434417   0.449904        0.0   0.008973   \n",
      "\n",
      "   var7(t-1)   var1(t)  \n",
      "1   0.782418  0.545045  \n",
      "2   0.782676  0.509006  \n",
      "3   0.774169  0.488550  \n",
      "4   0.778809  0.455597  \n",
      "5   0.798917  0.322555  \n"
     ]
    }
   ],
   "source": [
    "values = df_resample.values \n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "reframed.drop(reframed.columns[[8,9,10,11,12,13]], axis=1, inplace=True)\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17520, 1, 7) (17520,) (17068, 1, 7) (17068,)\n"
     ]
    }
   ],
   "source": [
    "values = reframed.values\n",
    "\n",
    "n_train_time = 365*24*2\n",
    "train = values[:n_train_time, :]\n",
    "test = values[n_train_time:, :]\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 17520 samples, validate on 17068 samples\n",
      "Epoch 1/300\n",
      " - 2s - loss: 0.0213 - val_loss: 0.0094\n",
      "Epoch 2/300\n",
      " - 2s - loss: 0.0108 - val_loss: 0.0088\n",
      "Epoch 3/300\n",
      " - 1s - loss: 0.0103 - val_loss: 0.0086\n",
      "Epoch 4/300\n",
      " - 2s - loss: 0.0101 - val_loss: 0.0085\n",
      "Epoch 5/300\n",
      " - 1s - loss: 0.0100 - val_loss: 0.0085\n",
      "Epoch 6/300\n",
      " - 1s - loss: 0.0099 - val_loss: 0.0085\n",
      "Epoch 7/300\n",
      " - 2s - loss: 0.0098 - val_loss: 0.0084\n",
      "Epoch 8/300\n",
      " - 2s - loss: 0.0098 - val_loss: 0.0084\n",
      "Epoch 9/300\n",
      " - 2s - loss: 0.0097 - val_loss: 0.0084\n",
      "Epoch 10/300\n",
      " - 2s - loss: 0.0097 - val_loss: 0.0085\n",
      "Epoch 11/300\n",
      " - 2s - loss: 0.0097 - val_loss: 0.0085\n",
      "Epoch 12/300\n",
      " - 2s - loss: 0.0096 - val_loss: 0.0084\n",
      "Epoch 13/300\n",
      " - 2s - loss: 0.0096 - val_loss: 0.0084\n",
      "Epoch 14/300\n",
      " - 2s - loss: 0.0095 - val_loss: 0.0084\n",
      "Epoch 15/300\n",
      " - 2s - loss: 0.0095 - val_loss: 0.0085\n",
      "Epoch 16/300\n",
      " - 2s - loss: 0.0096 - val_loss: 0.0083\n",
      "Epoch 17/300\n",
      " - 2s - loss: 0.0095 - val_loss: 0.0083\n",
      "Epoch 18/300\n",
      " - 2s - loss: 0.0095 - val_loss: 0.0083\n",
      "Epoch 19/300\n",
      " - 2s - loss: 0.0095 - val_loss: 0.0083\n",
      "Epoch 20/300\n",
      " - 2s - loss: 0.0094 - val_loss: 0.0084\n",
      "Epoch 21/300\n",
      " - 3s - loss: 0.0094 - val_loss: 0.0083\n",
      "Epoch 22/300\n",
      " - 3s - loss: 0.0094 - val_loss: 0.0083\n",
      "Epoch 23/300\n",
      " - 3s - loss: 0.0094 - val_loss: 0.0084\n",
      "Epoch 24/300\n",
      " - 2s - loss: 0.0094 - val_loss: 0.0083\n",
      "Epoch 25/300\n",
      " - 3s - loss: 0.0094 - val_loss: 0.0083\n",
      "Epoch 26/300\n",
      " - 3s - loss: 0.0094 - val_loss: 0.0083\n",
      "Epoch 27/300\n",
      " - 3s - loss: 0.0094 - val_loss: 0.0083\n",
      "Epoch 28/300\n",
      " - 3s - loss: 0.0094 - val_loss: 0.0083\n",
      "Epoch 29/300\n",
      " - 3s - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 30/300\n",
      " - 3s - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 31/300\n",
      " - 3s - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 32/300\n",
      " - 3s - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 33/300\n",
      " - 3s - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 34/300\n",
      " - 3s - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 35/300\n",
      " - 2s - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 36/300\n",
      " - 3s - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 37/300\n",
      " - 3s - loss: 0.0093 - val_loss: 0.0085\n",
      "Epoch 38/300\n",
      " - 3s - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 39/300\n",
      " - 3s - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 40/300\n",
      " - 2s - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 41/300\n",
      " - 2s - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 42/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 43/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 44/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0085\n",
      "Epoch 45/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 46/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 47/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 48/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 49/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0085\n",
      "Epoch 50/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0085\n",
      "Epoch 51/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 52/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 53/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 54/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 55/300\n",
      " - 3s - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 56/300\n",
      " - 3s - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 57/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 58/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0083\n",
      "Epoch 59/300\n",
      " - 2s - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 60/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0084\n",
      "Epoch 61/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0084\n",
      "Epoch 62/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 63/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0084\n",
      "Epoch 64/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 65/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0084\n",
      "Epoch 66/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 67/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 68/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 69/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0084\n",
      "Epoch 70/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0084\n",
      "Epoch 71/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 72/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 73/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 74/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 75/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 76/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 77/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0082\n",
      "Epoch 78/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 79/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 80/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0082\n",
      "Epoch 81/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 82/300\n",
      " - 2s - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 83/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0083\n",
      "Epoch 84/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 85/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 86/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 87/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 88/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 89/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 90/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 91/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 92/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 93/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 94/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 95/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 96/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 97/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 98/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 99/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0081\n",
      "Epoch 100/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0082\n",
      "Epoch 101/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 102/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0082\n",
      "Epoch 103/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 104/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 105/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0081\n",
      "Epoch 106/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0081\n",
      "Epoch 107/300\n",
      " - 2s - loss: 0.0090 - val_loss: 0.0081\n",
      "Epoch 108/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 109/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0082\n",
      "Epoch 110/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 111/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0082\n",
      "Epoch 112/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 113/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 114/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 115/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 116/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 117/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 118/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 119/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0082\n",
      "Epoch 120/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0082\n",
      "Epoch 121/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 122/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 123/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 124/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 125/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 126/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 127/300\n",
      " - 2s - loss: 0.0089 - val_loss: 0.0082\n",
      "Epoch 128/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 129/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 130/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 131/300\n",
      " - 3s - loss: 0.0088 - val_loss: 0.0080\n",
      "Epoch 132/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 133/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 134/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 135/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 136/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 137/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0080\n",
      "Epoch 138/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 139/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 140/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 141/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0080\n",
      "Epoch 142/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0080\n",
      "Epoch 143/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0080\n",
      "Epoch 144/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 145/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 146/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 147/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0081\n",
      "Epoch 148/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 149/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0080\n",
      "Epoch 150/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 151/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0081\n",
      "Epoch 152/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0081\n",
      "Epoch 153/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 154/300\n",
      " - 2s - loss: 0.0088 - val_loss: 0.0080\n",
      "Epoch 155/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 156/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 157/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 158/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 159/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 160/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 161/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0081\n",
      "Epoch 162/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 163/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 164/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0081\n",
      "Epoch 165/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 166/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 167/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 168/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 169/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 170/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 171/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 172/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 173/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 174/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 175/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 176/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 177/300\n",
      " - 2s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 178/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 179/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 180/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 181/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 182/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 183/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 184/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 185/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 186/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 187/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 188/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 189/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0079\n",
      "Epoch 190/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 191/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 192/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 193/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 194/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 195/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 196/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 197/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0080\n",
      "Epoch 198/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 199/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0079\n",
      "Epoch 200/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 201/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0079\n",
      "Epoch 202/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0079\n",
      "Epoch 203/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 204/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 205/300\n",
      " - 2s - loss: 0.0086 - val_loss: 0.0079\n",
      "Epoch 206/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 207/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 208/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 209/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0080\n",
      "Epoch 210/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 211/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 212/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 213/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 214/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 215/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0080\n",
      "Epoch 216/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 217/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 218/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 219/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 220/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0080\n",
      "Epoch 221/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 222/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 223/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 224/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 225/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 226/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 227/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 228/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 229/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 230/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 231/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 232/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0080\n",
      "Epoch 233/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 234/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 235/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0080\n",
      "Epoch 236/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 237/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 238/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 239/300\n",
      " - 2s - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 240/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 241/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 242/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0080\n",
      "Epoch 243/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 244/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 245/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 246/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 247/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0080\n",
      "Epoch 248/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 249/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 250/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0080\n",
      "Epoch 251/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0080\n",
      "Epoch 252/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 253/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 254/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 255/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 256/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 257/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 258/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0079\n",
      "Epoch 259/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 260/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 261/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 262/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 263/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 264/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 265/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 266/300\n",
      " - 2s - loss: 0.0084 - val_loss: 0.0080\n",
      "Epoch 267/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 268/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 269/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 270/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 271/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 272/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 273/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 274/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 275/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 276/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 277/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 278/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 279/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 280/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0079\n",
      "Epoch 281/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 282/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0079\n",
      "Epoch 283/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0079\n",
      "Epoch 284/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 285/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 286/300\n",
      " - 2s - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 287/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 288/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0079\n",
      "Epoch 289/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 290/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 291/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0079\n",
      "Epoch 292/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0079\n",
      "Epoch 293/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0079\n",
      "Epoch 294/300\n",
      " - 2s - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 295/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0079\n",
      "Epoch 296/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 297/300\n",
      " - 2s - loss: 0.0082 - val_loss: 0.0079\n",
      "Epoch 298/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0079\n",
      "Epoch 299/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 300/300\n",
      " - 2s - loss: 0.0083 - val_loss: 0.0080\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.575\n",
      "Test MSE: 0.331\n",
      "Test MAE: 0.426\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200,activation='relu',input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history = model.fit(train_X, train_y, epochs=300, batch_size=70, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], 7))\n",
    "inv_yhat = np.concatenate((yhat, test_X[:, -6:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = np.concatenate((test_y, test_X[:, -6:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "mse =mean_squared_error(inv_y, inv_yhat)\n",
    "print('Test MSE: %.3f' % mse)\n",
    "mae =mean_absolute_error(inv_y, inv_yhat)\n",
    "print('Test MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
